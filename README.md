# ğŸ¤– B.T. 2.0 - Vanguard-Class Voice Assistant

> **"Trust me, Pilot. I will not lose you again."** â€” BT-7274

A Titanfall 2's Titan voice assistant powered by OpenRouter, featuring dual personality modes (BT and Scorch) with advanced wake-word detection, speech recognition, TTS synthesis, and intelligent command execution.
<p align="center">
  <img src="https://media1.tenor.com/m/BnaAWfRhrO0AAAAC/titanfall-2.gif" alt="Titanfall 2">
</p>


## ğŸ“‹ Table of Contents

- [What is B.T. 2.0?](#what-is-bt-20)
- [Quick Start](#quick-start)
- [Installation & Setup](#installation--setup)
- [API Configuration](#api-configuration)
- [Pipeline Architecture](#pipeline-architecture)
- [Folder Structure](#folder-structure)
- [Decision Logic](#decision-logic)
- [Features & Capabilities](#features--capabilities)
- [Command Reference](#command-reference)
- [Debugging & Scaling](#debugging--scaling)
- [Troubleshooting](#troubleshooting)

---

## What is B.T. 2.0?

B.T. 2.0 is a **voice-driven desktop assistant** designed to control your computer through natural language commands. It features:

- **Dual AI Personalities**: Switch between BT-7274 (logical, mission-focused Vanguard) and Scorch (aggressive, thermite-loving Ogre)
- **Wake-Word Detection**: Always listening for "Hey BT" via Picovoice Porcupine
- **Speech Recognition**: Google Speech-to-Text for command parsing
- **Voice Synthesis**: Piper TTS with dual voice models for BT and Scorch
- **LLM Integration**: OpenAI API for intent classification and conversational fallback
- **Smart Command Execution**: 40+ system, media, web, and utility commands

> **Pilot Protocol 3 Engaged**: Protect the civilian population. Secure the area. Link with the Titan.

---

## Quick Start

### Prerequisites
- **Python 3.9+**
- **Windows 10/11** (uses `win32gui`, `psutil`, etc.)
- **Microphone** for audio input
- **Internet connection** (for APIs and TTS)

### Installation in 5 Minutes

```bash
# 1. Clone or download the project
cd "c:\Work\important stuff\B.T. 2.0"

# 2. Create virtual environment
python -m venv venv
venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Download NLTK data (automatic on first run, but you can pre-download)
python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab')"

# 5. Configure config.json (see next section)

# 6. Run the assistant
python main.py
```

---

## Installation & Setup

### Step 1: Install Dependencies

Create a `requirements.txt` with:

```txt
# Speech & Audio
SpeechRecognition==3.10.0
sounddevice==0.4.6
soundfile==0.12.1
pydub==0.25.1
gTTS==2.3.2
piper-tts==1.2.0
pvporcupine==3.0.2

# LLM & Translation
openai==1.3.0
deep-translator==1.11.4

# System & Control
pyautogui==0.9.53
psutil==5.9.6
Pillow==10.0.0
win32-setctime==1.1.0
pyperclip==1.8.2
python-Levenshtein==0.21.1

# Spotify Integration
spotipy==2.22.1

# Audio Control
pycaw==20240128

# GUI & Data
nltk==3.8.1
requests==2.31.0
```

```bash
pip install -r requirements.txt
```

### Step 2: Prepare Voice Models

The project uses **Piper TTS** for voice synthesis. You need:

1. **Download Piper binary** from [rhasspy/piper releases](https://github.com/rhasspy/piper/releases)
   - Extract to `piper/` folder
   - Should have: `piper.exe`, `espeak-ng-data/`, `pkgconfig/`

2. **Download Voice Models**:
   - BT Voice: `en_US-lessac-medium.onnx` (place in `Titanfall2/BT7274/`)
   - Scorch Voice: `en_GB-alan-medium.onnx` (place in `Titanfall2/ScorchAI/`)

3. **espeak-ng** is bundled with Piper (no separate install needed)

### Step 3: Wake-Word Model

The `Hey_Bt.ppn` file is **Picovoice Porcupine's** keyword model. It's provided in the repoâ€”no additional setup needed unless you want to custom-train a new model.
Just change the api key that you'll find in the `config.json` and in  `main.py`for porcupine.

---

## API Configuration

### config.json Setup

Create or edit `config.json` with your API keys and settings:

```json
{
  "paths": {
    "piper_exe": "piper/piper.exe",
    "bt_voice_model": "Titanfall2/BT7274/BT7274.onnx",
    "scorch_voice_model": "Titanfall2/ScorchAI/ScorchAI.onnx",
    "tts_cache_dir": "tts_cache"
  },
  "tts_settings": {
    "piper_timeout": 30,
    "sentences_per_chunk": 2,
    "use_multiprocessing_threshold": 2,
    "max_concurrent_piper": 2
  },
  "audio": {
    "default_input_device_name": "YOUR_MIC_NAME",
    "default_output_device_name": "YOUR_SPEAKERS_NAME",
    "listen_duration": 7
  },
  "llm_service": {
    "api_key": "sk-proj-YOUR_OPENAI_API_KEY",
    "model": "openai/gpt-3.5-turbo",
    "site_url": "http://localhost",
    "site_name": "TitanAssistant"
  },
  "vision_service": {
    "model": "qwen/qwen2.5-vl-32b-instruct:free",
    "default_prompt": "What is in this image?",
    "read_prompt": "Extract all text from this image"
  },
  "spotify": {
    "client_id": "YOUR_SPOTIFY_CLIENT_ID",
    "client_secret": "YOUR_SPOTIFY_CLIENT_SECRET",
    "redirect_uri": "http://localhost:8888/callback"
  },
  "commands": [
    // See Command Reference section
  ],
  "dialogue_pools": {
    "startup": ["BT online. Standing by for mission parameters."],
    "shutdown": ["Good luck, Pilot. See you on the Frontier."],
    "ptt_ack": ["Acknowledged."],
    "confirmation": ["Awaiting confirmation, Pilot."],
    "error": ["Unable to comply."]
  }
}
```

### API Keys Explained

| API | Purpose | Cost | Setup |
|-----|---------|------|-------|
| **OpenAI** | Intent classification & conversational LLM | ~$0.15/1k tokens | Get key at [platform.openai.com](https://platform.openai.com) |
| **Picovoice** | Wake-word detection (Porcupine) | Free tier available | Access key in code (limited but functional) |
| **Spotify** | Music control & now-playing info | Free with account | [spotify-dev.com](https://developer.spotify.com) |
| **Google Speech-to-Text** | Transcription (free via SpeechRecognition lib) | ~$0.006/15min | Built-in, no key needed |

> **Caution**: Store API keys in environment variables in production:
> ```python
> import os
> api_key = os.getenv('OPENAI_API_KEY')
> ```

---

## Pipeline Architecture

### Audio Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MAIN LOOP                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Wake-Word Detection (Porcupine)                             â”‚
â”‚     â””â”€â†’ Listening continuously for "Hey BT"                     â”‚
â”‚         (8kHz, 16-bit PCM)                                      â”‚
â”‚                                                                  â”‚
â”‚  2. PTT Acknowledged                                            â”‚
â”‚     â””â”€â†’ Play "ptt_ack" sound (cached BT voice)                  â”‚
â”‚                                                                  â”‚
â”‚  3. Listen for Command                                          â”‚
â”‚     â””â”€â†’ Record up to 7 seconds of audio                         â”‚
â”‚     â””â”€â†’ Stop on silence detected                                â”‚
â”‚                                                                  â”‚
â”‚  4. Speech-to-Text (Google)                                     â”‚
â”‚     â””â”€â†’ Convert AudioData â†’ Query string                        â”‚
â”‚                                                                  â”‚
â”‚  5. Command Processing                                          â”‚
â”‚     â”œâ”€â†’ Exact Keyword Match? YES â†’ Execute (Fast)              â”‚
â”‚     â”œâ”€â†’ No Match? â†’ LLM Intent Classification                   â”‚
â”‚     â”‚   â””â”€â†’ Classify intent â†’ Map to command type               â”‚
â”‚     â”œâ”€â†’ Still No Match? â†’ Conversational LLM fallback           â”‚
â”‚     â””â”€â†’ Execute Action (speak, open app, control media, etc.)   â”‚
â”‚                                                                  â”‚
â”‚  6. Text-to-Speech (Piper) [PIPELINED]                          â”‚
â”‚     â”œâ”€â†’ Split response into chunks (2 sentences/chunk)          â”‚
â”‚     â”œâ”€â†’ Launch Piper processes (max 2 concurrent)               â”‚
â”‚     â”œâ”€â†’ Play chunks as they complete (don't wait for all)       â”‚
â”‚     â””â”€â†’ Cache results for reuse                                 â”‚
â”‚                                                                  â”‚
â”‚  Loop Back to Step 1                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### TTS Pipelining Strategy

**Problem**: Generating full voice responses takes 2-5 seconds per 30 seconds of audio.

**Solution**: **Multiprocessing + Streaming Playback**

```python
# Example: Response = "First sentence. Second sentence. Third sentence."
# Chunks = ["First sentence.", "Second sentence.", "Third sentence."]

# Traditional (Sequential) - Wait 3s total
chunk1_path = generate(chunks[0])  # 1s
play(chunk1_path)                   # 1s
chunk2_path = generate(chunks[1])  # 1s
play(chunk2_path)                   # 1s
chunk3_path = generate(chunks[2])  # 1s
play(chunk3_path)                   # 1s
# Total: 6 seconds âš ï¸

# Pipelined (Parallel + Streaming) - Wait ~2s total
Process(generate, chunks[0])  # Start immediately
Process(generate, chunks[1])  # Start immediately
play(chunk1_path)            # 1s (while chunk2 still generating)
# chunk2 finishes during chunk1 playback
play(chunk2_path)            # 1s (while chunk3 still generating)
play(chunk3_path)            # 1s
# Total: ~2 seconds âœ…
```

### Caching System

- **Dialogue pool entries** (startup, shutdown, etc.) cached by mode + phrase hash
- **Generated TTS** stored as `.wav` in `tts_cache/`
- **Reuse on repeat commands** (e.g., "What time is it?" always uses cached response)
- **Auto-cleanup**: Temporary files deleted after playback

---

## Folder Structure

```
B.T. 2.0/
â”œâ”€â”€ main.py                 # Main assistant code
â”œâ”€â”€ config.json            # Configuration (API keys, paths, commands)
â”œâ”€â”€ Hey_Bt.ppn             # Porcupine wake-word model
â”œâ”€â”€ Overlyrics.py          # Lyrics display module (optional)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚
â”œâ”€â”€ piper/                 # Piper TTS binary & data
â”‚   â”œâ”€â”€ piper.exe          # TTS executable
â”‚   â”œâ”€â”€ espeak-ng-data/    # Phoneme data (60+ languages)
â”‚   â””â”€â”€ pkgconfig/         # Library configuration
â”‚
â”œâ”€â”€ Titanfall2/            # Voice models & project data
â”‚   â”œâ”€â”€ BT7274/
â”‚   â”‚   â”œâ”€â”€ BT7274.onnx         # BT's voice model (primary)
â”‚   â”‚   â”œâ”€â”€ BT7274.onnx.json    # Model metadata
â”‚   â”‚   â””â”€â”€ voices.json         # Voice configuration
â”‚   â”œâ”€â”€ ScorchAI/
â”‚   â”‚   â”œâ”€â”€ ScorchAI.onnx       # Scorch's voice model
â”‚   â”‚   â”œâ”€â”€ ScorchAI.onnx.json  # Model metadata
â”‚   â”‚   â”œâ”€â”€ voices.json         # Voice configuration
â”‚   â”‚   â””â”€â”€ test.py             # Testing script
â”‚   â””â”€â”€ Todo.md            # Development notes
â”‚
â”œâ”€â”€ tts_cache/             # Generated audio cache
â”‚   â”œâ”€â”€ cache_bt_a1b2c3d4e5f6.wav
â”‚   â”œâ”€â”€ cache_scorch_f6e5d4c3b2a1.wav
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ __pycache__/           # Python bytecode (auto-generated)
```

---

## Decision Logic

### How Does B.T. Choose What to Do?

```python
# DECISION TREE (in process_command)

query = "play darude sandstorm"  # User says this

# LEVEL 1: Mode Switch Detection (Highest Priority)
if "switch mode" in query or "switch to scorch" in query:
    â†’ Change current_mode
    â†’ Execute immediately
    â†’ Return (skip remaining levels)

# LEVEL 2: Exact Keyword Matching (Fast Path)
for command in CONFIG['commands']:
    for keyword in command['keywords']:
        if query.startswith(keyword):  # Exact match wins!
            â†’ Execute command
            â†’ Return (skip remaining levels)
            
# LEVEL 3: LLM Intent Classification (Smart Path)
# Only if no exact match found AND LLM available
intent = llm_client.classify_intent(query)
# intent = "play_music" (from a predefined set of ~25 intents)

intent_to_command_map = {
    "play_music": {"type": "media.play_music", ...},
    "set_volume": {"type": "system.set_volume", ...},
    "open_website": {"type": "web.open", ...},
    # ... 25+ more mappings
}

command = intent_to_command_map.get(intent)
if command:
    â†’ Execute command with parsed data
    â†’ Return

# LEVEL 4: Conversational Fallback (Weakest)
# If still no match, hand off to conversational LLM
response = llm_client.chat(query, system_prompt=bt_system_prompt)
speak(response)  # "I'm not familiar with that request, Pilot."
```

### Why This Order?

1. **Mode switching** is critical (immediate response needed)
2. **Exact keywords** are fastest (~5ms, no LLM call)
3. **LLM classification** handles variations ("play music" vs "start playing a song")
4. **Conversational fallback** handles questions and casual chat

### Example Decision Traces

| User Query | Decision Path | Result |
|------------|---------------|--------|
| "Switch to Scorch" | Level 1 | Switches mode, speaks confirmation |
| "Play Hacking to the Gate" | Level 2 â†’ Keyword "play" matches | Executes `media.play_music` |
| "Can you play some chill music?" | Level 3 â†’ LLM: "play_music" | Executes `media.play_music` with data |
| "Are you alive?" | Level 4 â†’ Conversational LLM | BT responds philosophically |

---

## Features & Capabilities

### Core Features

| Feature | Status | Notes |
|---------|--------|-------|
| **Wake-Word Detection** | âœ… | "Hey BT" always listening (Porcupine) |
| **Speech Recognition** | âœ… | Google Speech-to-Text (free, accurate) |
| **Voice Synthesis** | âœ… | Dual voices (BT & Scorch), pipelined TTS |
| **Command Execution** | âœ… | 40+ commands (system, media, web, utility) |
| **Intent Classification** | âœ… | LLM-powered, 25+ intent categories |
| **Conversational AI** | âœ… | OpenAI GPT for fallback responses |
| **Spotify Integration** | âœ… | Play/pause, skip, current track, volume |
| **Vision Analysis** | âœ… | Screenshot + OCR (QwEN Vision model) |
| **Multi-Mode** | âœ… | Switch between BT (logical) and Scorch (aggressive) |
| **TTS Caching** | âœ… | Reuse generated audio for fast responses |
| **Audio Device Config** | âœ… | Choose input/output devices from config |

### Command Categories

#### ğŸµ **Media Control** (7 commands)
- `play [song]` â†’ Spotify playback
- `pause` / `resume` â†’ Toggle music
- `next` / `skip` â†’ Next track
- `previous` / `back` â†’ Previous track
- `volume [0-100]` â†’ Set volume
- `mute` / `unmute` â†’ Mute audio
- `what's playing` / `now playing` â†’ Current track info

#### ğŸŒ **Web & App** (8 commands)
- `open [website]` â†’ Launch URL (Chrome, Firefox, Edge)
- `search [query]` â†’ Google search
- `open [app]` â†’ Launch application
- `close [app]` â†’ Terminate application
- `switch to [app]` â†’ Bring window to front

#### ğŸ’» **System Control** (12 commands)
- `set brightness [0-100]` â†’ Display brightness
- `set volume [0-100]` â†’ Master volume
- `wifi on` / `wifi off` â†’ Network control
- `lock computer` â†’ Lock screen
- `shutdown` / `restart` â†’ System reboot
- `list windows` â†’ Show open windows
- `system status` â†’ CPU, RAM, disk usage
- `top processes` â†’ Running applications
- `set input device [name]` â†’ Microphone selection
- `set output device [name]` â†’ Speaker selection

#### ğŸ”§ **Utilities** (8 commands)
- `translate [text] to [language]` â†’ Google Translate + TTS
- `analyze screen` â†’ Vision LLM analysis + OCR
- `open lyrics` / `close lyrics` â†’ Display song lyrics
- `type [text]` â†’ Keyboard input
- `what time is it` â†’ Current time
- `what's the date` â†’ Current date
- `tell me a joke` â†’ Humor (configurable)
- `switch mode` â†’ Toggle BT â†” Scorch

#### ğŸ“‹ **Window Control** (5 commands)
- `minimize [window]` â†’ Reduce window
- `maximize [window]` â†’ Expand window
- `close [window]` â†’ Terminate window
- `next tab` â†’ Switch browser tab
- `list windows` â†’ Show all open windows

---

## Command Reference

### Awesome Commands to Try ğŸ®

Here are the **coolest commands** that showcase B.T. 2.0's power:

#### 1. **Vision Analysis** (Requires Vision API)
```
"Analyze my screen"
"Read what's on the screen"
```
â†’ Captures screenshot, compresses it, sends to QwEN Vision LLM, and reads the result aloud.
**Use Case**: Quick reading of notifications, website content, or screen info without looking.

#### 2. **Smart Search**
```
"Search for titanfall 3 news"
"Open youtube"
```
â†’ Detects intent, opens browser, performs search (or just opens site).
**Use Case**: Hands-free web browsing while working.

#### 3. **Music Playback with Intent**
```
"Play darude sandstorm"
"I want to hear some lofi beats"
```
â†’ LLM classifies as music intent, launches Spotify, searches for song/playlist.
**Use Case**: Voice control without knowing exact song titles.

#### 4. **Dual Personality Switch**
```
"Switch to Scorch"
```
â†’ Changes voice, system prompt, and personality instantly.
- **BT**: Logical, concise, mission-focused
- **Scorch**: Aggressive, combat-oriented, laconic

**Use Case**: Roleplay, different moods, or entertainment.

#### 5. **Real-Time Translation**
```
"Translate hello world to spanish"
```
â†’ Translates via Google Translate, plays audio in target language via gTTS.
**Use Case**: Quick language learning or communication.

#### 6. **Screen Content Reading** (OCR)
```
"Read my screen"
```
â†’ Captures image, compresses, sends to Vision LLM, extracts & speaks all text.
**Use Case**: Accessibility, reading fine print, verifying passwords visually.

#### 7. **Smart Window Management**
```
"Switch to Chrome"
"Minimize Spotify"
"Close this window"
```
â†’ Fuzzy-matches open windows by name, performs action.
**Use Case**: Window organization without touching mouse.

#### 8. **Dynamic System Status**
```
"How's my system?"
"What are the top processes?"
```
â†’ Reads CPU, RAM, disk, running processes; speaks in BT's voice.
**Use Case**: Quick diagnostics while gaming or working.

#### 9. **Automatic Lyrics Display**
```
"Show me the lyrics"
"Close the lyrics"
```
â†’ Launches external lyrics viewer (OVLyrics) or closes it.
**Use Case**: Karaoke-style singing along.

#### 10. **Mode-Aware Caching**
```
(Ask for the time twice rapidly)
```
â†’ First call: generates voice file
â†’ Second call: reuses cached file (instant response)
**Use Case**: See the difference in speed for repeated requests.

---

## Debugging & Scaling

### Performance Optimization

#### 1. **TTS Bottleneck** (Most Common)
**Problem**: Voice synthesis slow (2-5s per response)

**Solutions**:
```python
# In config.json
{
  "tts_settings": {
    "sentences_per_chunk": 2,           # â†‘ Increase = fewer chunks = faster
    "use_multiprocessing_threshold": 2, # â†“ Lower = use parallel sooner
    "max_concurrent_piper": 2           # â†‘ Increase = more parallel processes
  }
}
```

**Benchmark**:
- `sentences_per_chunk: 1` â†’ Many small chunks â†’ Better streaming, slower overall
- `sentences_per_chunk: 3` â†’ Fewer large chunks â†’ Faster overall, chunkier playback
- **Sweet spot**: 2 (balance of both)

#### 2. **LLM Latency** (Intent Classification)
**Problem**: First response slow (LLM API call takes 1-2s)

**Solutions**:
- Use exact keywords instead (5ms vs 1500ms)
- Increase `max_tokens` in config only if needed
- Fallback to conversational if intent classification fails
- Cache common intents in local config

#### 3. **Speech Recognition Timeout**
**Problem**: "Waiting for microphone" stalls input

**Solutions**:
```python
# Adjust in code:
listen_timeout = 5      # Max wait for speech to START
duration = 7            # Max length of one command
```

#### 4. **Memory Leak in TTS Cache**
**Problem**: `tts_cache/` grows unbounded

**Solutions**:
```python
# Add periodic cleanup (monthly)
import glob
for f in glob.glob("tts_cache/*.wav"):
    if time.time() - os.path.getmtime(f) > 30*24*3600:  # 30 days
        os.unlink(f)
```

---

### Scaling to Enterprise

**Challenge**: Running 100+ instances, centralized audio storage

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   B.T. 2.0 Instance (Local PC)              â”‚
â”‚   â”œâ”€ Wake-word detection                    â”‚
â”‚   â”œâ”€ Command parsing (local LLM/rules)      â”‚
â”‚   â””â”€ UI/Audio output                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ (API calls only)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Central Server                            â”‚
â”‚   â”œâ”€ OpenAI API (shared quota)              â”‚
â”‚   â”œâ”€ TTS service (Piper + caching)          â”‚
â”‚   â”œâ”€ Shared command config                  â”‚
â”‚   â””â”€ Metrics & logging                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
1. Move TTS to server (GPU-accelerated)
2. Cache results in Redis
3. Distribute config via API endpoint
4. Use async/await for all I/O

---

### Debugging Checklist

| Issue | Diagnosis | Fix |
|-------|-----------|-----|
| Wake-word not triggering | Check mic levels: `sd.query_devices()` | Increase `sensitivities` in code (0.6 â†’ 0.8) |
| Transcription says "None" | Check internet, Google SR rate limits | Wait 30s, try again |
| Voice response too fast/slow | Piper output speed | Edit `piper_generate_worker` speech rate |
| LLM response nonsensical | System prompt mismatch | Verify `bt_system_prompt` in `get_llm_response()` |
| Audio device selection failing | Device names mismatch config | Run `sd.query_devices()` to find exact names |
| Cache files not cleaning up | Temp file deletion failing | Check disk permissions on `tts_cache/` |
| "Piper timeout" error | Audio file too long | Lower `SENTENCES_PER_CHUNK` or increase `PIPER_TIMEOUT` |

---

## Troubleshooting

### Common Issues & Fixes

#### ğŸ”Š "No Audio Device Found"
```python
# Diagnose:
import sounddevice as sd
print(sd.query_devices())

# Look for your device, get index
# Update config.json:
{
  "audio": {
    "default_input_device_name": "YOUR_EXACT_MIC_NAME",
    "default_output_device_name": "YOUR_EXACT_SPEAKER_NAME"
  }
}
```

#### ğŸ¤« "Microphone Sensitivity Too High/Low"
```python
# In calibrate_microphone():
MINIMUM_ENERGY_THRESHOLD = 300  # â†‘ Increase = less sensitive
                                 # â†“ Decrease = more sensitive
```

#### ğŸ“¡ "API Key Invalid"
```json
{
  "llm_service": {
    "api_key": "sk-proj-..."  // Must start with "sk-proj-" or "sk-"
  }
}
```
Get from: https://platform.openai.com/api-keys

#### ğŸ™ï¸ "Wake-Word Not Detecting"
```python
# In main.py, check Porcupine initialization:
porcupine = pvporcupine.create(
    access_key="YOUR_KEY",
    keyword_paths=["Hey_Bt.ppn"],
    sensitivities=[0.6]  # â†‘ 0.6 = default, 0.9 = very sensitive, 0.3 = strict
)
```

#### â±ï¸ "Commands Timing Out"
```python
# In config.json:
{
  "tts_settings": {
    "piper_timeout": 30  # â†‘ Increase for long responses
  }
}
```

#### ğŸ”¥ "GPU Memory Issues" (if using GPU TTS)
```bash
# Force CPU-only Piper:
export CUDA_VISIBLE_DEVICES=""
python main.py
```

---

## Advanced Configuration

### Custom Commands

Add to `config.json`:

```json
{
  "commands": [
    {
      "name": "Play Music",
      "type": "media.play_music",
      "keywords": ["play"],
      "ack": "Starting playback."
    },
    {
      "name": "Custom Greeting",
      "type": "custom.greet",
      "keywords": ["hey", "hello"],
      "ack": "What's up, Pilot?"
    }
  ]
}
```

Then handle in `execute_action()`:
```python
elif action_type == "custom.greet":
    speak("Acknowledged. Standing by for orders.")
```

### Custom Dialogue Pools

```json
{
  "dialogue_pools": {
    "startup": [
      "BT online. Standing by for mission parameters.",
      "Vanguard class Titan, ready for deployment."
    ],
    "error": [
      "Unable to comply.",
      "That request exceeds my current parameters."
    ]
  }
}
```

---

## Performance Metrics

On Windows 11 (i7-12700K, 16GB RAM):

| Operation | Time | Notes |
|-----------|------|-------|
| Wake-word detection | < 100ms | Per audio frame |
| Command transcription | 0.5-2s | Network-dependent |
| Exact keyword match | ~5ms | No LLM call |
| LLM intent classification | 1-3s | API latency |
| TTS generation (sequential) | 2-5s | Per response |
| TTS generation (pipelined 2 chunks) | 1-2s | Parallel processing |
| Spotify command latency | 0.5-1.5s | API + playback start |

---

## References & Resources

- **Piper TTS**: https://github.com/rhasspy/piper
- **Picovoice Porcupine**: https://picovoice.ai/
- **OpenAI API**: https://platform.openai.com/
- **Spotipy Docs**: https://spotipy.readthedocs.io/
- **SpeechRecognition**: https://github.com/Uberi/speech_recognition

---

## License & Credits

**B.T. 2.0** is a fan project inspired by Titanfall 2. All Titanfall 2 assets and characters belong to Respawn Entertainment / EA Games.

- **BT-7274**: "Guardian of the Frontier"
- **Scorch**: "Area Denial Specialist"
- **Pilot**: That's you, Titan. Let's move.

---

## Join the Frontier ğŸ®

> "Stay sharp, Pilot. We've got a lot of work to do." â€” BT-7274

For issues, feature requests, or Titanfall memes, reach out or open an issue on GitHub.

**Mission Status**: ACTIVE âœ…

---

**Last Updated**: January 8, 2026
**Version**: 2.0 (Vanguard Protocol)
